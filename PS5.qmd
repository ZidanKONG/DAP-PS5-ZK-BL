---
title: "30538 Problem Set 5: Web Scraping"
author: "Boya Lin & Zidan Kong"
date: "11/06/2024"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Boya Lin, boya1
    - Partner 2 (name and cnet ID): Zidan Kong, zidank
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*BL\*\* \*\*ZK\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*1\*\* Late coins left after submission: \*\*1\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
# set the url
url = 'https://oig.hhs.gov/fraud/enforcement/'
# make a get request
response = requests.get(url)
# convert into a soup object
soup = BeautifulSoup(response.text, 'lxml')

# initialize list to store data
titles = []
dates = []
categories = []
links = []

for action in soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12'):
    title_tag = action.find('h2', class_='usa-card__heading')
    # Use 'N/A' if title not found
    title = title_tag.text.strip() if title_tag else 'N/A'
    titles.append(title)

    date_tag = action.find('span', class_='text-base-dark padding-right-105')
    date = date_tag.text.strip() if date_tag else 'N/A'
    dates.append(date)

    category_tag = action.find(
        'li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_tag.text.strip() if category_tag else 'N/A'
    categories.append(category)

    link_tag = action.find('a')
    if link_tag and link_tag.get('href'):
        link = 'https://oig.hhs.gov' + link_tag.get('href')
    else:
        link = 'N/A'
    links.append(link)

# create a DataFrame with the scraped data
data = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links
})

# display the head of the DataFrame
print(data.head())
```


### 2. Crawling (PARTNER 1)

```{python}
# initialize list to store agency
agencies = []

# iterate over each enforcement action link
for link in links:
    action_response = requests.get(link)
    action_soup = BeautifulSoup(action_response.text, 'lxml')

    # locate the 'Agency:' label and extract the agency name by accessing the next sibling
    agency_tag = action_soup.find('span', text='Agency:')
    if agency_tag:
        agency_text = agency_tag.find_next_sibling(text=True).strip('" ').split(';')[-1].strip()
    else:
        agency_text = 'N/A'
    
    agencies.append(agency_text)
    time.sleep(1)

data['Agency'] = agencies
print(data.head())
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

I will use a while loop since i am going to iterate page by page, i will break the loop if it does not meet the date condition. I will define the while loop to run based on if the url still exists.

Define function enforcement_scraper(month, year):

    IF year < 2013 THEN
        PRINT "Please restrict to year >= 2013."
        RETURN None

    Create empty lists for later append:
        titles = empty list
        dates = empty list
        categories = empty list
        links = empty list
        agencies = empty list

    base_url = "https://oig.hhs.gov/fraud/enforcement/"
    target_date = datetime of input

    session = requests.Session()

    SET page_count = 0

    WHILE url:
        page_count += 1
        PRINT "Fetching page {page_count}..." for checking scraping process

        response = session.GET(url)
        soup = BeautifulSoup(response.text, "lxml")

        actions = FIND all action items in soup
        IF actions is empty
            BREAK

        valid_date_page = False

        FOR EACH action find detail:
            title, date, category, full_link based on previous questions

            try:
                action_date = parse_action_date(date)

                IF action_date < target_date THEN
                    CONTINUE
                ELSE
                    valid_date_page = True

            APPEND title
            APPEND date
            APPEND category 
            APPEND full_link OR "N/A"

            agency_text = fetch_agency_details(session, full_link)
            APPEND agency_text TO agencies

            SLEEP for 1 seconds

        IF valid_date_page IS False THEN
            PRINT "No relevant actions found on this page. Ending scraping."
            BREAK

        url = find_next_page_url(soup, base_url)
        IF url IS NOT None THEN
            Update url to next page
            PRINT "Moving to next page..."
        ELSE
            PRINT "No next page found. Ending scraping."
            BREAK out of the loop


    data = pd.DataFrame(titles, dates, categories, links, agencies)
    filename = "enforcement_actions_{year}_{month}.csv"
    SAVE data TO CSV file named filename WITHOUT index
    PRINT "Data saved to {filename}"

    RETURN data

 Referencing for understanding Pseudo-Code,https://builtin.com/data-science/pseudocode


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
def enforcement_scraper(month, year):
    # check if the input year is valid
    if year < 2013:
        print("Please restrict to year >= 2013.")
        return None

    # initialize lists to store data
    titles = []
    dates = []
    categories = []
    links = []
    agencies = []

    # define the base URL
    url = "https://oig.hhs.gov/fraud/enforcement/"
    target_date = datetime(year, month, 1)

    # set up a requests session for efficiency
    session = requests.Session()

    # loop through pages until no more pages are found
    page_count = 0
    while url:
        page_count += 1
        print(f"Fetching page {page_count}...")  # print the current page number

        response = session.get(url)
        soup = BeautifulSoup(response.text, "lxml")

        # find all actions on the page
        actions = soup.find_all("li", class_="usa-card card--list pep-card--minimal mobile:grid-col-12")
        if not actions:
            break

        # flag to check if all actions on the page are before the target date
        page_has_relevant_actions = False

        for action in actions:
            # extract title, date, category, and link for each action
            title_tag = action.find("h2", class_="usa-card__heading")
            title = title_tag.text.strip() if title_tag else "N/A"

            date_tag = action.find("span", class_="text-base-dark padding-right-105")
            date_text = date_tag.text.strip() if date_tag else "N/A"

            category_tag = action.find(
                "li", class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1"
            )
            category = category_tag.text.strip() if category_tag else "N/A"

            link_tag = action.find("a")
            link = link_tag["href"] if link_tag else None
            full_link = f'https://oig.hhs.gov{link}' if link and link.startswith("/") else link

            # parse the action date and filter based on target_date
            try:
                action_date = datetime.strptime(date_text, "%B %d, %Y")
                if action_date < target_date:
                    continue  # skip actions before the target date
                else:
                    page_has_relevant_actions = True  # found at least one relevant action on this page
            except ValueError:
                continue  # skip actions with invalid date format

            # append data for actions that meet the date criteria
            titles.append(title)
            dates.append(date_text)
            categories.append(category)
            links.append(full_link or "N/A")

            # if agency details are critical, make an additional request here
            if full_link:
                action_response = session.get(full_link)
                action_soup = BeautifulSoup(action_response.text, "lxml")
                agency_tag = action_soup.find("span", string="Agency:")
                agency_text = (
                    agency_tag.find_next_sibling(string=True).strip('" ').split(":")[-1].strip()
                    if agency_tag else "N/A"
                )
                agencies.append(agency_text)

            # pause to avoid hitting the server too frequently
            time.sleep(1)

        # check if this page had no relevant actions after the target date
        if not page_has_relevant_actions:
            print("No relevant actions found on this page. Ending scraping.")
            break

        # find the link to the next page
        next_page = soup.find("a", class_="pagination-next")
        if next_page:
            next_link = next_page["href"]
            url = urljoin(url, next_link)
            print("Moving to next page...")
        else:
            print("No next page found. Ending scraping.")
            break

    # create a DataFrame with the collected data
    data = pd.DataFrame({
        "Title": titles,
        "Date": dates,
        "Category": categories,
        "Link": links,
        "Agency": agencies
    })

    # save the DataFrame to a CSV file
    filename = f"enforcement_actions_{year}_{month}.csv"
    data.to_csv(filename, index=False)
    print(f"Data saved to {filename}")

    return data
```


```{python}
# collect the enforcement actions since January 2023
data_2023 = enforcement_scraper(1, 2023)

num_actions_2023 = len(data_2023)
print(f'Total enforcement actions from January 2023: {num_actions_2023}')
earliest_action_2023 = data_2023.sort_values(by='Date').iloc[0]
print(f'Earliest enforcement action scraped: {earliest_action_2023}')
```

* c. Test Partner's Code (PARTNER 1)

```{python}
# collect the enforcement actions since January 2021
data_2021 = enforcement_scraper(1, 2021)

num_actions_2021 = len(data_2021)
print(f'Total enforcement actions from January 2021: {num_actions_2021}')
earliest_action_2021 = data_2021.sort_values(by='Date').iloc[0]
print(f'Earliest enforcement action scraped: {earliest_action_2021}')
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
enforcement_actions_2021 = pd.read_csv('enforcement_actions_2021_1.csv') 

# convert to datetime format
enforcement_actions_2021['Date'] = pd.to_datetime(enforcement_actions_2021['Date'], format='%B %d, %Y')

# extract 'Year-Month' for grouping
enforcement_actions_2021['Year-Month'] = enforcement_actions_2021['Date'].dt.to_period('M')

# convert 'Year-Month' to string for Altair compatibility
enforcement_actions_2021['Year-Month'] = enforcement_actions_2021['Year-Month'].astype(str)

# aggregate by Year-Month and count the number of enforcement actions
enforcement_counts = enforcement_actions_2021.groupby('Year-Month').size().reset_index(name='Enforcement Actions')

# plot the data using Altair
chart = alt.Chart(enforcement_counts).mark_line().encode(
    x='Year-Month:T', 
    y='Enforcement Actions:Q', 
    tooltip=['Year-Month:T', 'Enforcement Actions:Q']
).properties(
    title='Number of Enforcement Actions Over Time (Since January 2021)',
    width=500,
    height=300
)

chart.show()
```


### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
enforcement_actions_2021 = pd.read_csv('enforcement_actions_2021_1.csv') 
#filter data
enforcement_2category = enforcement_actions_2021[enforcement_actions_2021['Category'].isin(['Criminal and Civil Actions', 'State Enforcement Agencies'])]

#structure date
enforcement_2category['Date'] = pd.to_datetime(enforcement_2category['Date'], format='%B %d, %Y')

# extract 'Year-Month' for grouping
enforcement_2category['Year-Month'] = enforcement_2category['Date'].dt.to_period('M')

# convert 'Year-Month' to string for Altair compatibility
enforcement_2category['Year-Month'] = enforcement_2category['Year-Month'].astype(str)

# Count the occurrences of each Category by Date
enforcement_2category = enforcement_2category.groupby(['Date', 'Category']).size().reset_index(name='Count')
# Create the Altair line chart
alt.Chart(enforcement_2category).mark_line().encode(
    x=alt.X('Date:T', axis=alt.Axis(format='%Y-%m', title='Date')),
    y=alt.Y('Count:Q', title='Counts'),
    color='Category:N',
    tooltip=['Date:T', 'Category:N', 'Count:Q']
).properties(
    title='Enforcement Actions Over Time by 2 Categories',
    width=600,
    height=400
)

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```