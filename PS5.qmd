---
title: "30538 Problem Set 5: Web Scraping"
author: "Boya Lin & Zidan Kong"
date: "11/06/2024"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Boya Lin, boya1
    - Partner 2 (name and cnet ID): Zidan Kong, zidank
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*BL\*\* \*\*ZK\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*1\*\* Late coins left after submission: \*\*1\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from datetime import datetime
import requests
from bs4 import BeautifulSoup

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
url = 'https://oig.hhs.gov/fraud/enforcement/'
# make a get request
response = requests.get(url)
# convert into a soup object
soup = BeautifulSoup(response.text, 'lxml')

# initialize list to store data
titles = []
dates = []
categories = []
links = []

for action in soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12'):
    title_tag = action.find('h2', class_='usa-card__heading')
    # Use 'N/A' if title not found
    title = title_tag.text.strip() if title_tag else 'N/A'
    titles.append(title)

    date_tag = action.find('span', class_='text-base-dark padding-right-105')
    date = date_tag.text.strip() if date_tag else 'N/A'
    dates.append(date)

    category_tag = action.find(
        'li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_tag.text.strip() if category_tag else 'N/A'
    categories.append(category)

    link_tag = action.find('a')
    link = link_tag['href'] if link_tag else None
    full_link = f'https://oig.hhs.gov{link}' if link and link.startswith(
        '/') else link
    links.append(full_link or 'N/A')

# create a DataFrame with the scraped data
data = pd.DataFrame({
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links
})

# display the head of the DataFrame
print(data.head())
```


### 2. Crawling (PARTNER 1)

```{python}
# initialize list to store agency
agencies = []

# iterate over each enforcement action link
for link in links:
    action_response = requests.get(link)
    action_soup = BeautifulSoup(action_response.text, 'lxml')

    # locate the 'Agency:' label and extract the agency name by accessing the next sibling
    agency_tag = action_soup.find('span', text='Agency:')
    if agency_tag:
        agency_text = agency_tag.find_next_sibling(text=True).strip('" ').split(';')[-1].strip()
    else:
        agency_text = 'N/A'
    
    agencies.append(agency_text)
    time.sleep(1)

data['Agency'] = agencies
print(data.head())
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

Define function enforcement_scraper(month, year):
    
    If year < 2013 or year == 2013 and month < 1:
        Print "Please restrict to year >= 2013."
        Return None 

    Create an empty list titles
    Create an empty list dates
    Create an empty list categories
    Create an empty list links
    Create an empty list agencies

    Set url to "https://oig.hhs.gov/fraud/enforcement/"

    Set want_date afetr the input date

    While True:
        Get request to url
        Parse the information using BeautifulSoup and store in name of 'soup'

        Find all 'li' elements with class "usa-card card--list pep-card--minimal mobile:grid-col-12" in soup
        If no actions are found, Break the loop 

        For each action in actions:
            
            Find title_tag 
            Get text else na

            Find date_tag
            Get text else na

            Find category_tag
            Get text else na

            Find link_tag
            If link exists:
                Format link as full_link
            Else:
                Append 'N/A' 
            
            try:
                strip date scraped from above
                if stripped_date , want_date:
                    continue
            except ValueError:
                continue

            Append title
            Append date
            Append category
            Append link

            Get request to links in full_link
            Parse the information using BeautifulSoup
            Find the element with text "Agency:"
            If agency element is found:
                Get agency name, then append to agencies list
            Else:
                Append 'N/A' to agencies list
            
            Wait for 1 second

        Find next page link in soup
        If next page link is found:
            Update url with the next page URL
        Else:
            Break the loop 

    Create a DataFrame data with columns:
        'Title': titles
        'Date': dates
        'Category': categories
        'Link': links
        'Agency': agencies

    Set filename as “enforcement_actions_ year_month.csv”
    Save data to filename without the index column

    Return data


 Refenrenicng for understanding Pseudo-Code

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
from urllib.parse import urljoin

def enforcement_scraper(month, year):
    # check if the input year is valid
    if year < 2013:
        print('Please restrict to year >= 2013.')
        return None

    # create empty lists to store data
    titles = []
    dates = []
    categories = []
    links = []
    agencies = []

    # define the base URL
    url = "https://oig.hhs.gov/fraud/enforcement/"

    # want date to filter actions date
    want_date = datetime(year, month, 1)

    # Loop through pages until no more pages are found
    while True:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'lxml')

        actions = soup.find_all('li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
        if not actions:
            break

        for action in actions:
            # Extract title, date, category, and link for each action
            title_tag = action.find('h2', class_='usa-card__heading')
            title = title_tag.text.strip() if title_tag else 'N/A'

            date_tag = action.find('span', class_='text-base-dark padding-right-105')
            date = date_tag.text.strip() if date_tag else 'N/A'

            category_tag = action.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest')
            category = category_tag.text.strip() if category_tag else 'N/A'

            link_tag = action.find('a')
            link = link_tag['href'] if link_tag else None
            full_link = f'https://oig.hhs.gov{link}' if link and link.startswith('/') else link

            # Get the date and filter based on want_date
            try:
                action_date = datetime.strptime(date, '%B %d, %Y')
                if action_date < want_date:
                    continue 
            except ValueError:
                continue  

            # Append the filtered data
            titles.append(title)
            dates.append(date)
            categories.append(category)
            links.append(full_link or 'N/A')

            # Request the enforcement action detail page to find the agency name
            action_response = requests.get(full_link)
            action_soup = BeautifulSoup(action_response.text, 'lxml')
            agency_tag = action_soup.find('span', text='Agency:')
            if agency_tag:
                agency_text = agency_tag.find_next_sibling(text=True).strip('" ').split(':')[-1].strip()
            else:
                agency_text = 'N/A'
            agencies.append(agency_text)

            # Pause to avoid hitting the server too frequently
            time.sleep(1)

        # Find the link to the next page using the `page` class
        next_page = soup.find('li', class_='page')
        if next_page:
            next_link = next_page.find('a')['href']
            url = urljoin('https://oig.hhs.gov/', next_link)
        else:
            break

    # Create a DataFrame with the filtered data
    data = pd.DataFrame({
        'Title': titles,
        'Date': dates,
        'Category': categories,
        'Link': links,
        'Agency': agencies
    })

    # Save the DataFrame to a CSV file
    filename = "enforcement_actions_ year_month.csv"
    data.to_csv(filename, index=False)

    return data
```

```{python}
# collect the enforcement actions since January 2023
data_2023 = enforcement_scraper(1, 2023)

num_actions_2023 = len(data_2023)
print(f'Total enforcement actions from January 2023: {num_actions_2023}')

earliest_action_2023 = data_2023.sort_values(by='Date').iloc[0]
print(f'Earliest enforcement action scraped: {earliest_action_2023}')
```

* c. Test Partner's Code (PARTNER 1)

```{python}
# collect the enforcement actions since January 2021
data_2021 = enforcement_scraper(1, 2021)

num_actions_2021 = len(data_2021)
print(f'Total enforcement actions from January 2021: {num_actions_2021}')

earliest_action_2021 = data_2021.sort_values(by='Date').iloc[0]
print(f'Earliest enforcement action scraped: {earliest_action_2021}')
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```